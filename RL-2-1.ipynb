{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Day2\n",
    "## 本章で学ぶ内容 \n",
    "* 行動評価の指標となる**価値（累積報酬）の定義**\n",
    "* 状態評価を**動的計画法で学習する手法と実装**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 価値（累計報酬）を定式化するにあたっての課題\n",
    "1. 将来の報酬が判明している必要がある  \n",
    "&rarr; 再帰的に表現することで解決  \n",
    "2. 将来の報酬の予測が、必ずあたるとしている  \n",
    "&rarr; 期待値として見積もることで解決  \n",
    "\n",
    "状態$s$において戦略$\\pi$に基づいて行動した結果得られる価値を$V_\\pi(s)$とし、  \n",
    "現在の状態の価値を期待値で表す。  \n",
    "  \n",
    "$V_\\pi(S_t)=E_\\pi[r_{t+1}+\\gamma V_\\pi(S_{t+1})]$   \n",
    "  \n",
    "ここで、$r$は即時報酬、$\\gamma$は割引率、$E[...]$は期待値を表す  \n",
    "添え字の$t+1$は、行動後に得られる価値を表している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bellman Equationによる定義\n",
    "期待値は、**行動確率**$\\pi(a|s)$と**遷移確率**$T(s'|s,a)$を導入して次式で表すことができる  \n",
    "$V_\\pi(S_t)=\\displaystyle\\sum_a\\pi(a|s)  \n",
    "\\displaystyle\\sum_aT(s'|s,a)(R(s,s')+\\gamma V_\\pi(s'))$  \n",
    "上記の式を<u>**Bellman Equation**</u>と呼ぶ  \n",
    "\n",
    "**遷移確率**: 状態sの時に行動aをとった際に、状態s'に遷移する確率。  \n",
    "**行動確率**: 状態sの時にaの行動をとる確率を戦略と呼び、特別に次式で表す  \n",
    "\n",
    "参考になったページ  \n",
    "https://gsnc.hatenablog.com/entry/2019/02/18/051713  \n",
    "https://qiita.com/MENDY/items/77e9ae4cdd7c6476b080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2つの前提によるBellman Equationの単純化\n",
    "1. 価値を最大とする行動をとる(Valueベース。戦略を基準とするPolicyベースもある)  \n",
    "2. MDP（マルコフ決定課程）である(R(s,s')はR(s)となる)  \n",
    " \n",
    "であるためBellman Equationは次式で表せる  \n",
    "$V(s)=R(s)+\\gamma max_a\\displaystyle\\sum_{s'}T(s'|s,a)V(s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 実装編\n",
    "### 環境\n",
    "\"up\" か \"down\"の行動を5回行う。upだと＋1、downだと-1され、5回終了後に4以上だと成功。3以下は失敗。  \n",
    "<div align=\"center\">\n",
    "<img src=stairs.png width=30%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 価値の定義\n",
    "$V(s)=R(s)+\\gamma max_a\\displaystyle\\sum_{s'}T(s'|s,a)V(s')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def V(s, gamma=0.99):\n",
    "    V = R(s) + gamma * max_V_on_next_state(s)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 報酬関数\n",
    "5回終了時に4以上で成功(happy end)か3以下で失敗(bad end)か。それ以外は0。\n",
    "（5回終了時にしか報酬は無い）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def R(s):\n",
    "    if s == \"happy_end\":\n",
    "        return 1\n",
    "    elif s == \"bad_end\":\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 価値の計算\n",
    "Valueベースの実装  \n",
    "$\\gamma max_a\\displaystyle\\sum_{s'}T(s'|s,a)V(s')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def max_V_on_next_state(s):\n",
    "    # If game end, expected value is 0.\n",
    "    if s in [\"happy_end\", \"bad_end\"]:\n",
    "        return 0\n",
    "\n",
    "    actions = [\"up\", \"down\"]\n",
    "    values = []\n",
    "    for a in actions:\n",
    "        # 遷移確率の計算（次ページで定義）\n",
    "        transition_probs = transit_func(s, a)\n",
    "        v = 0\n",
    "        for next_state in transition_probs:\n",
    "            prob = transition_probs[next_state]\n",
    "            # 遷移確率×遷移先の報酬\n",
    "            v += prob * V(next_state)\n",
    "        values.append(v)\n",
    "    # Valueベースなので、最大の価値のみ返す\n",
    "    return max(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 遷移確率の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def transit_func(s, a):\n",
    "     actions = s.split(\"_\")[1:]\n",
    "     LIMIT_GAME_COUNT = 5\n",
    "     HAPPY_END_BORDER = 4\n",
    "     # 選択した行動をとる確率を設定。\n",
    "     MOVE_PROB = 0.9\n",
    " \n",
    "     def next_state(state, action):\n",
    "         return \"_\".join([state, action])\n",
    " \n",
    "     if len(actions) == LIMIT_GAME_COUNT:\n",
    "         up_count = sum([1 if a == \"up\" else 0 for a in actions])\n",
    "         state = \"happy_end\" if up_count >= HAPPY_END_BORDER else \"bad_end\"\n",
    "         prob = 1.0\n",
    "         return {state: prob}\n",
    "     else:\n",
    "         opposite = \"up\" if a == \"down\" else \"down\"\n",
    "         return {\n",
    "             next_state(s, a): MOVE_PROB,\n",
    "             next_state(s, opposite): 1 - MOVE_PROB\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ## 確認\n",
    " 指定の状態の価値を確認する\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7880942034605892\n",
      "0.99\n",
      "-0.96059601\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "     print(V(\"state\"))\n",
    "     print(V(\"state_up_up_up_up_up\"))\n",
    "     print(V(\"state_down_down\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## キーワード\n",
    " * Bellman Equation\n",
    "   * ある状態における**価値（累計報酬）**の計算式\n",
    "   * $V_\\pi(S_t)=\\displaystyle\\sum_a\\pi(a|s)\\displaystyle\\sum_aT(s'|s,a)(R(s,s')+\\gamma V_\\pi(s'))$\n",
    " * 動的計画法\n",
    "   * 「モデルベース」の学習方法の一種\n",
    "     * モデルベース:遷移関数、報酬関数をベースに学習するモデルのこと\n",
    "     * モデルフリー:遷移関数、報酬関数が明確でない場合の手法。3章で学ぶ\n",
    "   * 動的計画法の内、価値反復法について学ぶ\n",
    " * 行動決定の方法\n",
    "   * Valueベース：**価値（累計報酬）**を最大化するように行動\n",
    "   * Policyベース：戦略に基づいて行動"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "ここまでの実装は、  \n",
    "取りうるすべての行動に対して価値を計算し、価値が最大となる行動  \n",
    "をとるようにしたが、  \n",
    "状態数が多い場合にすべての状態の計算を行うことは困難である。  \n",
    "\n",
    "そこで・・・\n",
    "<div align=\"center\">\n",
    "<img src=jouhou_hanran.png width=30%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 動的計画法（DP:Dynamic Programing）\n",
    "ナップサック問題などに使われる、アルゴリズム界隈の言葉。  \n",
    "以下、Wikipediaより引用\n",
    "\n",
    "> https://ja.wikipedia.org/wiki/動的計画法  \n",
    "> 細かくアルゴリズムが定義されているわけではなく、下記2条件を満たすアルゴリズムの総称である。  \n",
    "> 1.分割統治法：部分問題を解き、その結果を利用して、問題全体を解く  \n",
    "> 2.メモ化：部分問題の計算結果を再利用する  \n",
    ">   \n",
    "> 対象となる問題を帰納的に解く場合にくり返し出現する小さな問題例について、解を表に記録し表を埋めていく形で計算をすすめ、冗長な計算をはぶくアルゴリズムのことをいう。\n",
    "\n",
    "今回は、動的計画法により各状態の価値を計算する**<u>価値反復法</u>（Value Iteration）**と、\n",
    "戦略を学習する<u>**Policy Iteration**</u>を実装する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 価値反復法（Value Iteration）\n",
    "各状態の価値を、繰り返し計算を行うことで計算する。\n",
    "基礎方程式は、Bellman equationを変形した以下の式  \n",
    "$V_{i+1}(s)=max_a\\displaystyle\\sum_aT(s'|s,a)(R(s)+\\gamma V_i(s'))$  \n",
    "ここで、$i$は計算ステップを表しており、遷移先の価値は、ひとつ前の値（$V_i(s')$）を用いている。  \n",
    "$V_i(s)$の値の更新が閾値を下回るまで繰り返し計算を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "<img src=value.png width=50%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class Planner():\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.log = []\n",
    "\n",
    "    def initialize(self):\n",
    "        self.env.reset()\n",
    "        self.log = []\n",
    "\n",
    "    def plan(self, gamma=0.9, threshold=0.0001):\n",
    "        raise Exception(\"Planner have to implements plan method.\")\n",
    "    # 遷移確率の関数の定義\n",
    "    def transitions_at(self, state, action):\n",
    "        # 状態、行動のペアから確率を計算\n",
    "        transition_probs = self.env.transit_func(state, action)\n",
    "        for next_state in transition_probs:\n",
    "            prob = transition_probs[next_state]\n",
    "            reward, _ = self.env.reward_func(next_state)\n",
    "            yield prob, next_state, reward\n",
    "\n",
    "    #状態の行列\n",
    "    def dict_to_grid(self, state_reward_dict):\n",
    "        grid = []\n",
    "        for i in range(self.env.row_length):\n",
    "            row = [0] * self.env.column_length\n",
    "            grid.append(row)\n",
    "        for s in state_reward_dict:\n",
    "            grid[s.row][s.column] = state_reward_dict[s]\n",
    "\n",
    "        return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Plannerクラスを継承\n",
    "class ValuteIterationPlanner(Planner):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def plan(self, gamma=0.9, threshold=0.0001):\n",
    "        self.initialize()\n",
    "        actions = self.env.actions\n",
    "        V = {}\n",
    "        for s in self.env.states:\n",
    "            # Initialize each state's expected reward.\n",
    "            # 最初の価値は、0とする\n",
    "            V[s] = 0\n",
    "\n",
    "        while True:\n",
    "            delta = 0\n",
    "            self.log.append(self.dict_to_grid(V))\n",
    "            for s in V:\n",
    "                if not self.env.can_action_at(s):\n",
    "                    continue\n",
    "                expected_rewards = []\n",
    "                for a in actions:\n",
    "                    r = 0\n",
    "                    for prob, next_state, reward in self.transitions_at(s, a):\n",
    "                        #Bellman Equationの価値反復法への適用\n",
    "                        r += prob * (reward + gamma * V[next_state])\n",
    "                    expected_rewards.append(r)\n",
    "                #価値反復法なので、最大値のみを保持\n",
    "                max_reward = max(expected_rewards)\n",
    "                delta = max(delta, abs(max_reward - V[s]))\n",
    "                V[s] = max_reward\n",
    "\n",
    "            if delta < threshold:\n",
    "                break\n",
    "\n",
    "        V_grid = self.dict_to_grid(V)\n",
    "        return V_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Policy Iteration\n",
    "**価値計算**、**戦略更新**の繰り返しを、戦略が更新されなくなるまで行う。\n",
    "基礎方程式は、Bellman equationを変形した以下の式\n",
    "$V_\\pi(s)=\\displaystyle\\sum_a\\pi(a|s)\\displaystyle\\sum_aT(s'|s,a)(R(s,s')+\\gamma V_\\pi(s'))$  \n",
    "ここで、$i$は計算ステップを表しており、遷移先の価値は、ひとつ前の値（$V_i(s')$）を用いている。  \n",
    "$V_i(s)$の値の更新が閾値を下回るまで繰り返し計算を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "<img src=policy.png>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
