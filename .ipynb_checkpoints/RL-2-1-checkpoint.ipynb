{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Day2\n",
    "## 本日の内容 \n",
    "* 行動評価の指標となる**価値（累積報酬）の定義**\n",
    "* 状態評価を**動的計画法で学習する手法と実装**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Day1の復習\n",
    "* 機械学習では、**環境**が**マルコフ決定過程（MDP）** に従うことを仮定する。\n",
    "  * MDP：「遷移先の状態は直前の状態とそこでの行動のみに依存する」\n",
    "* MDPでは**即時報酬 $r$** は「直前の**状態 $s$** と**遷移先の状態 $s'$**」に依存する。\n",
    "* 未来の報酬の総和を見積もった値を **価値 $V$** と呼ぶ。\n",
    "* 見積もる際に、将来の即時報酬は **割引率 $\\gamma$** を乗じて評価する。\n",
    "  * $V_t = r_{t+1}+\\gamma r_{t+2}+\\gamma^2r_{t+3}\\cdot\\cdot\\cdot$\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./doc/mdp.PNG\" width=800 alt=\"mdp.PNG\"/>\n",
    "  <div align=\"center\">MDPの構成要素とその関係</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 価値を定式化（実装）するにあたっての課題\n",
    "1. 将来の報酬が判明している必要がある  \n",
    "&rarr; 再帰的に表現することで解決  \n",
    "2. 将来の報酬の予測が、必ずあたるとしている  \n",
    "&rarr; 報酬・遷移先の発生確率を考慮して期待値で表すことで解決  \n",
    "\n",
    "状態 $s$ において戦略 $\\pi$ に基づいて行動した結果得られる価値を $V_\\pi(s)$ とし、  \n",
    "現在の状態の価値を期待値で表す。  \n",
    "  \n",
    "$V_\\pi(s_t)=E_\\pi[r_{t+1}+\\gamma V_\\pi(s_{t+1})]$   \n",
    "  \n",
    "$r$は即時報酬、$\\gamma$は割引率、$E[...]$は期待値を表す  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bellman Equationによる定義\n",
    "期待値は、**行動確率**$\\pi(a|s)$と**遷移確率**$T(s'|s,a)$を導入して次式で表すことができる  \n",
    "  \n",
    "$V_\\pi(s_t)=\\displaystyle\\sum_a\\pi(a|s)  \n",
    "\\displaystyle\\sum_{s'}T(s'|s,a)(R(s,s')+\\gamma V_\\pi(s'))$  \n",
    "上記の式を<u>**Bellman Equation**</u>と呼ぶ  \n",
    "\n",
    "**遷移確率**: 状態sの時に行動aをとった際に、状態s'に遷移する確率。  \n",
    "**行動確率**: 状態sの時にaの行動をとる確率を戦略と呼び、特別に次式で表す  \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=janken.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2つの前提によるBellman Equationの単純化\n",
    "1. 価値を最大とする行動をとる（**Valueベース**。戦略を基準とするPolicyベースもある）  \n",
    "2. 報酬は、現在の状態にのみ依存する（$R(s,s')$は$R(s)$となる）  \n",
    " \n",
    "であるためBellman Equationは次式で表せる \n",
    "  \n",
    "$V(s)=R(s)+\\gamma max_a\\displaystyle\\sum_{s'}T(s'|s,a)V(s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 実装編\n",
    "### 環境\n",
    "\"up\" か \"down\"の行動を5回行う。upだと＋1、downだと-1され、5回終了後に4以上だと成功。3以下は失敗。  \n",
    "また、選択した行動と逆の行動を行う確率を設定する。（$T(s'|s,a)$の理解のため）  \n",
    "<div align=\"center\">\n",
    "<img src=stairs.png width=50%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "<img src=kaidan_suusiki.png width=50%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 価値の定義\n",
    "ある状態$s$の価値$V(s)$の実装    \n",
    "$V(s)=R(s)+\\gamma max_a\\displaystyle\\sum_{s'}T(s'|s,a)V(s')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def V(s, gamma=0.99):\n",
    "    V = R(s) + gamma * max_V_on_next_state(s)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 報酬関数\n",
    "5回終了時に4以上で成功(happy end)か3以下で失敗(bad end)か。それ以外は0。\n",
    "（5回終了時にしか報酬は無い）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def R(s):\n",
    "    if s == \"happy_end\":\n",
    "        return 1\n",
    "    elif s == \"bad_end\":\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 価値の計算\n",
    "Valueベースの実装  \n",
    "$\\gamma max_a\\displaystyle\\sum_{s'}T(s'|s,a)V(s')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def max_V_on_next_state(s):\n",
    "    # If game end, expected value is 0.\n",
    "    if s in [\"happy_end\", \"bad_end\"]:\n",
    "        return 0\n",
    "\n",
    "    actions = [\"up\", \"down\"]\n",
    "    values = []\n",
    "    # 取りうる行動全てに対して、遷移確率を考慮して期待値を見積もる\n",
    "    for a in actions:\n",
    "        # 遷移確率の計算（次ページで定義）\n",
    "        transition_probs = transit_func(s, a)\n",
    "        v = 0\n",
    "        for next_state in transition_probs:\n",
    "            prob = transition_probs[next_state]\n",
    "            # 遷移確率×遷移先の報酬\n",
    "            v += prob * V(next_state)\n",
    "        values.append(v)\n",
    "    # Valueベースなので、最大の価値のみ返す\n",
    "    return max(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 遷移確率の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def transit_func(s, a):\n",
    "     actions = s.split(\"_\")[1:]\n",
    "     LIMIT_GAME_COUNT = 5\n",
    "     HAPPY_END_BORDER = 4\n",
    "     # 選択した行動をとる確率を設定。\n",
    "     MOVE_PROB = 0.9\n",
    " \n",
    "     def next_state(state, action):\n",
    "         return \"_\".join([state, action])\n",
    " \n",
    "    # 取りうる行動とその確率の対を返す\n",
    "     if len(actions) == LIMIT_GAME_COUNT:\n",
    "         up_count = sum([1 if a == \"up\" else 0 for a in actions])\n",
    "         state = \"happy_end\" if up_count >= HAPPY_END_BORDER else \"bad_end\"\n",
    "         prob = 1.0\n",
    "         return {state: prob}\n",
    "     else:\n",
    "         opposite = \"up\" if a == \"down\" else \"down\"\n",
    "         return {\n",
    "             next_state(s, a): MOVE_PROB,\n",
    "             next_state(s, opposite): 1 - MOVE_PROB\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ## 状態の価値がどうなっているか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7880942034605892\n",
      "0.9068026334400001\n",
      "0.29689909357877997\n",
      "-0.96059601\n",
      "-0.9702989999999999\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "     print(V(\"state\"))\n",
    "     print(V(\"state_up_up\"))\n",
    "     print(V(\"state_down\"))\n",
    "     print(V(\"state_down_down\"))\n",
    "     print(V(\"state_down_down_down\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## キーワード\n",
    " * Bellman Equation\n",
    "   * ある状態における**価値（累計報酬）**の計算式\n",
    "   * $V_\\pi(S_t)=\\displaystyle\\sum_a\\pi(a|s)\\displaystyle\\sum_aT(s'|s,a)(R(s,s')+\\gamma V_\\pi(s'))$\n",
    " * 動的計画法\n",
    "   * 「モデルベース」の学習方法の一種\n",
    "     * モデルベース:遷移関数、報酬関数をベースに学習するモデルのこと\n",
    "     * モデルフリー:遷移関数、報酬関数が明確でない場合の手法。3章で学ぶ\n",
    "   * 動的計画法の内、価値反復法について学ぶ\n",
    " * 行動決定の方法\n",
    "   * Valueベース：**価値（累計報酬）**を最大化するように行動\n",
    "   * Policyベース：戦略に基づいて行動"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "以上の実装は、  \n",
    "取りうるすべての行動に対して価値を計算し、価値が最大となる行動  \n",
    "をとるようにしたが、  \n",
    "状態数が多い場合にすべての状態の計算を行うことは困難である。  \n",
    "\n",
    "そこで・・・\n",
    "<div align=\"center\">\n",
    "<img src=jouhou_hanran.png width=30%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 動的計画法（DP:Dynamic Programing）\n",
    "ナップサック問題、部分和問題などに使われる、アルゴリズム界隈の言葉。  \n",
    "以下、Wikipediaより引用\n",
    "\n",
    "> https://ja.wikipedia.org/wiki/動的計画法  \n",
    "> 細かくアルゴリズムが定義されているわけではなく、下記2条件を満たすアルゴリズムの総称である。  \n",
    "> 1.分割統治法：部分問題を解き、その結果を利用して、問題全体を解く  \n",
    "> 2.メモ化：部分問題の計算結果を再利用する  \n",
    ">   \n",
    "> 対象となる問題を帰納的に解く場合にくり返し出現する小さな問題例について、解を表に記録し表を埋めていく形で計算をすすめ、冗長な計算をはぶくアルゴリズムのことをいう。\n",
    "\n",
    "今回は、動的計画法により各状態の価値を計算する**<u>価値反復法</u>（Value Iteration）**と、\n",
    "戦略を学習する<u>**Policy Iteration**</u>を実装する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 価値反復法（Value Iteration）\n",
    "各状態の価値を、繰り返し計算を行うことで計算する。\n",
    "基礎方程式は、Bellman equationを変形した以下の式  \n",
    "$V_{i+1}(s)=max_a\\displaystyle\\sum_aT(s'|s,a)(R(s)+\\gamma V_i(s'))$  \n",
    "ここで、$i$は計算ステップを表しており、遷移先の価値は、ひとつ前の値（$V_i(s')$）を用いている。  \n",
    "$V_i(s)$の値の更新が閾値を下回るまで繰り返し計算を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "<img src=value.png width=50%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class Planner():\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.log = []\n",
    "\n",
    "    def initialize(self):\n",
    "        self.env.reset()\n",
    "        self.log = []\n",
    "\n",
    "    def plan(self, gamma=0.9, threshold=0.0001):\n",
    "        raise Exception(\"Planner have to implements plan method.\")\n",
    "    # 遷移確率の関数の定義\n",
    "    def transitions_at(self, state, action):\n",
    "        # 状態、行動のペアから確率を計算\n",
    "        transition_probs = self.env.transit_func(state, action)\n",
    "        for next_state in transition_probs:\n",
    "            prob = transition_probs[next_state]\n",
    "            reward, _ = self.env.reward_func(next_state)\n",
    "            yield prob, next_state, reward\n",
    "\n",
    "    #状態の行列\n",
    "    def dict_to_grid(self, state_reward_dict):\n",
    "        grid = []\n",
    "        for i in range(self.env.row_length):\n",
    "            row = [0] * self.env.column_length\n",
    "            grid.append(row)\n",
    "        for s in state_reward_dict:\n",
    "            grid[s.row][s.column] = state_reward_dict[s]\n",
    "\n",
    "        return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Plannerクラスを継承\n",
    "class ValuteIterationPlanner(Planner):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def plan(self, gamma=0.9, threshold=0.0001):\n",
    "        self.initialize()\n",
    "        actions = self.env.actions\n",
    "        V = {}\n",
    "        for s in self.env.states:\n",
    "            # Initialize each state's expected reward.\n",
    "            # 最初の価値は、0とする\n",
    "            V[s] = 0\n",
    "\n",
    "        while True:\n",
    "            delta = 0\n",
    "            self.log.append(self.dict_to_grid(V))\n",
    "            for s in V:\n",
    "                if not self.env.can_action_at(s):\n",
    "                    continue\n",
    "                expected_rewards = []\n",
    "                for a in actions:\n",
    "                    r = 0\n",
    "                    for prob, next_state, reward in self.transitions_at(s, a):\n",
    "                        #Bellman Equationの価値反復法への適用\n",
    "                        r += prob * (reward + gamma * V[next_state])\n",
    "                    expected_rewards.append(r)\n",
    "                #価値反復法なので、最大値のみを保持\n",
    "                max_reward = max(expected_rewards)\n",
    "                delta = max(delta, abs(max_reward - V[s]))\n",
    "                V[s] = max_reward\n",
    "\n",
    "            if delta < threshold:\n",
    "                break\n",
    "\n",
    "        V_grid = self.dict_to_grid(V)\n",
    "        return V_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Policy Iteration\n",
    "**価値計算**、**戦略更新**の繰り返しを、戦略が更新されなくなるまで行う。  \n",
    "基礎方程式は、Bellman equationを変形した以下の式  \n",
    "  \n",
    "$V_\\pi(s)=\\displaystyle\\sum_a\\pi(a|s)\\displaystyle\\sum_aT(s'|s,a)(R(s,s')+\\gamma V_\\pi(s'))$  \n",
    "更新前の戦略$\\pi$を用いて価値を計算し、価値が最大となる行動をとるように、戦略$\\pi$を更新する。\n",
    "戦略$\\pi$の更新がなくなるまで繰り返し計算を続ける。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "<img src=policy.png>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## 今日出てきたキーワード\n",
    " * Bellman Equation\n",
    "   * ある状態における**価値（累計報酬）**の式\n",
    "   * $V_\\pi(S_t)=\\displaystyle\\sum_a\\pi(a|s)\\displaystyle\\sum_aT(s'|s,a)(R(s,s')+\\gamma V_\\pi(s'))$\n",
    " * 動的計画法\n",
    "   * 「モデルベース」の学習方法の一種\n",
    "     * モデルベース:遷移関数、報酬関数をベースに学習するモデルのこと\n",
    "     * モデルフリー:遷移関数、報酬関数が明確でない場合の手法。3章で学ぶ\n",
    " * 行動決定の方法\n",
    "   * Valueベース：**価値（累計報酬）**を最大化するように行動\n",
    "   * Policyベース：戦略に基づいて行動"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
